{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlVC5j9gdd99"
   },
   "source": [
    "# Наивный Байесовский Классификатор для классификации спам-сообщений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "i35VBQfFdd-E",
    "ExecuteTime": {
     "end_time": "2023-07-26T04:43:51.050068100Z",
     "start_time": "2023-07-26T04:43:46.104512600Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитаем файл (разделителем здесь выступает символ табуляции)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "A1b5QrS5dd-F",
    "outputId": "ad0e2dbf-f406-41ae-8cda-9e043efb2f8b",
    "ExecuteTime": {
     "end_time": "2023-07-26T04:43:56.838881300Z",
     "start_time": "2023-07-26T04:43:56.599889600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  Label                                                SMS\n0   ham  Go until jurong point, crazy.. Available only ...\n1   ham                      Ok lar... Joking wif u oni...\n2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n3   ham  U dun say so early hor... U c already then say...\n4   ham  Nah I don't think he goes to usf, he lives aro...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Label</th>\n      <th>SMS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_data = pd.read_csv('data/SMSSpamCollection.zip', header=None, sep='\\t', names=['Label', 'SMS'])\n",
    "sms_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, сколько объектов каждого класса присутствует в датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "p7_2JzUvdd-G",
    "outputId": "6167fcf9-f386-48cf-b2fe-4ca8012835c1",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-07-26T05:56:41.808780700Z",
     "start_time": "2023-07-26T05:56:41.692782100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        SMS\nLabel      \nham    4825\nspam    747",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SMS</th>\n    </tr>\n    <tr>\n      <th>Label</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>ham</th>\n      <td>4825</td>\n    </tr>\n    <tr>\n      <th>spam</th>\n      <td>747</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_data.groupby('Label').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQG2a4SVdd-H"
   },
   "source": [
    "### Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удаляем символы, не являющиеся буквами, приводим тексты SMS к нижнему регистру, разбиваем строки на слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "T9c0M4vhdd-I",
    "ExecuteTime": {
     "end_time": "2023-07-26T05:57:30.354354600Z",
     "start_time": "2023-07-26T05:57:30.258356300Z"
    }
   },
   "outputs": [],
   "source": [
    "sms_data_clean = sms_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Jd-UcYg6dd-I",
    "ExecuteTime": {
     "end_time": "2023-07-26T05:57:32.635140500Z",
     "start_time": "2023-07-26T05:57:32.563139900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0    [go, until, jurong, point, crazy, available, o...\n1                       [ok, lar, joking, wif, u, oni]\n2    [free, entry, in, 2, a, wkly, comp, to, win, f...\n3    [u, dun, say, so, early, hor, u, c, already, t...\n4    [nah, i, don, t, think, he, goes, to, usf, he,...\nName: SMS, dtype: object"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_data_clean['SMS'] = sms_data_clean['SMS'].str.replace('\\W+', ' ', regex=True)\n",
    "\n",
    "sms_data_clean['SMS'] = sms_data_clean['SMS'].str.replace('\\s+', ' ', regex=True).str.strip()\n",
    "sms_data_clean['SMS'] = sms_data_clean['SMS'].str.lower()\n",
    "sms_data_clean['SMS'] = sms_data_clean['SMS'].str.split()\n",
    "\n",
    "sms_data_clean['SMS'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "PZCmBLCLdd-L",
    "outputId": "e37750a2-4124-4733-9bb2-0e7b057e4171",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-07-26T05:57:38.700343700Z",
     "start_time": "2023-07-26T05:57:38.633314700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "ham     86.593683\nspam    13.406317\nName: Label, dtype: float64"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_data_clean['Label'].value_counts() / sms_data_clean.shape[0] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50oksBrrdd-L"
   },
   "source": [
    "### Разделение на обучающую и тестовую выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-26T05:57:52.071456200Z",
     "start_time": "2023-07-26T05:57:51.785543300Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = sms_data_clean.sample(frac=0.8, random_state=42)\n",
    "test_data = sms_data_clean.drop(train_data.index)\n",
    "\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "WU_YwDxgdd-M",
    "outputId": "d7897ce6-95c9-4be3-f269-5009666c7f3e",
    "ExecuteTime": {
     "end_time": "2023-07-26T05:57:54.833828Z",
     "start_time": "2023-07-26T05:57:54.816793300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "ham     86.698071\nspam    13.301929\nName: Label, dtype: float64"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['Label'].value_counts() / train_data.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "B_WG0yuqdd-M",
    "outputId": "c3c9a44e-9ee8-42f4-d6b6-5522898cb267",
    "ExecuteTime": {
     "end_time": "2023-07-26T05:57:58.132181100Z",
     "start_time": "2023-07-26T05:57:58.123185900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(4458, 2)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "fbrlnUxfdd-N",
    "outputId": "2bdc65d7-0b76-4c28-dae7-d1fe620e6a59",
    "ExecuteTime": {
     "end_time": "2023-07-26T05:58:02.953119300Z",
     "start_time": "2023-07-26T05:58:02.944120200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "ham     86.175943\nspam    13.824057\nName: Label, dtype: float64"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['Label'].value_counts() / test_data.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "dHM1P3h2dd-N",
    "outputId": "4b2a0eba-0468-4e7d-e1dd-f091728cdecc",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-07-26T05:58:07.985893400Z",
     "start_time": "2023-07-26T05:58:07.973895200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(1114, 2)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим, что и в обучающей, и в тестовой выборке содержится примерно 86-87% спама – как и в нашем оригинальном датасете."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVmTrhNSdd-O"
   },
   "source": [
    "### Список слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаём список всех слов, встречающихся в обучающей выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ahLFpWI3dd-O",
    "ExecuteTime": {
     "end_time": "2023-07-26T05:58:27.110914700Z",
     "start_time": "2023-07-26T05:58:25.672926200Z"
    }
   },
   "outputs": [],
   "source": [
    "vocabulary = list(set(train_data['SMS'].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "eSN6U0QLdd-O",
    "outputId": "5dcb7d6f-900f-483c-e896-4ef69459dced",
    "ExecuteTime": {
     "end_time": "2023-07-26T05:58:31.270283400Z",
     "start_time": "2023-07-26T05:58:31.257247200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['deliver',\n 'dbuk',\n 'sponsors',\n 'hyde',\n 'singing',\n 'pmt',\n 'director',\n 'costs',\n 'wiskey']"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[11:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "MegzBwCFdd-P",
    "outputId": "fa6a956c-5602-4193-8925-6ecb43b5b719",
    "ExecuteTime": {
     "end_time": "2023-07-26T05:58:36.426173900Z",
     "start_time": "2023-07-26T05:58:36.410174700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "7816"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xs_mok2wdd-P"
   },
   "source": [
    "### Рассчитаем частоты слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого SMS-сообщения посчитаем, сколько раз в нём встречается каждое слово."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "vER-xPhXdd-P",
    "ExecuteTime": {
     "end_time": "2023-07-26T05:59:45.126051400Z",
     "start_time": "2023-07-26T05:58:49.531324400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   07808726822  087187262701  jokes  antelope  m6  doing  thurs  smells  10k  \\\n0            0             0      0         0   0      0      0       0    0   \n1            0             0      0         0   0      0      0       0    0   \n2            0             0      0         0   0      0      0       0    0   \n3            0             0      0         0   0      0      0       0    0   \n4            0             0      0         0   0      0      0       0    0   \n\n   happen  ...  bookedthe  tonite  hamper  sent  fishhead  disconnected  \\\n0       0  ...          0       0       0     0         0             0   \n1       0  ...          0       0       0     0         0             0   \n2       0  ...          0       0       0     0         0             0   \n3       0  ...          0       0       0     0         0             0   \n4       0  ...          0       0       0     0         0             0   \n\n   tomorrow  180  2wks  diff  \n0         0    0     0     0  \n1         0    0     0     0  \n2         0    0     0     0  \n3         0    0     0     0  \n4         0    0     0     0  \n\n[5 rows x 7816 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>07808726822</th>\n      <th>087187262701</th>\n      <th>jokes</th>\n      <th>antelope</th>\n      <th>m6</th>\n      <th>doing</th>\n      <th>thurs</th>\n      <th>smells</th>\n      <th>10k</th>\n      <th>happen</th>\n      <th>...</th>\n      <th>bookedthe</th>\n      <th>tonite</th>\n      <th>hamper</th>\n      <th>sent</th>\n      <th>fishhead</th>\n      <th>disconnected</th>\n      <th>tomorrow</th>\n      <th>180</th>\n      <th>2wks</th>\n      <th>diff</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 7816 columns</p>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts_per_sms = pd.DataFrame([\n",
    "    [row[1].count(word) for word in vocabulary]\n",
    "    for _, row in train_data.iterrows()], columns=vocabulary)\n",
    "\n",
    "word_counts_per_sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим частоты каждого слова в обучающий датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "dNc8Juygdd-P",
    "ExecuteTime": {
     "end_time": "2023-07-26T06:06:28.703627200Z",
     "start_time": "2023-07-26T06:06:28.488627600Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = pd.concat([train_data, word_counts_per_sms], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "FxcXHWgqdd-Q",
    "outputId": "cda89d7c-4596-4dc7-e274-01afb214c54c",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-07-26T06:06:29.941998900Z",
     "start_time": "2023-07-26T06:06:29.890001Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  Label                                                SMS  07808726822  \\\n0   ham  [squeeeeeze, this, is, christmas, hug, if, u, ...            0   \n1   ham  [and, also, i, ve, sorta, blown, him, off, a, ...            0   \n2   ham  [mmm, thats, better, now, i, got, a, roast, do...            0   \n3   ham  [mm, have, some, kanji, dont, eat, anything, h...            0   \n4   ham  [so, there, s, a, ring, that, comes, with, the...            0   \n\n   087187262701  jokes  antelope  m6  doing  thurs  smells  ...  bookedthe  \\\n0             0      0         0   0      0      0       0  ...          0   \n1             0      0         0   0      0      0       0  ...          0   \n2             0      0         0   0      0      0       0  ...          0   \n3             0      0         0   0      0      0       0  ...          0   \n4             0      0         0   0      0      0       0  ...          0   \n\n   tonite  hamper  sent  fishhead  disconnected  tomorrow  180  2wks  diff  \n0       0       0     0         0             0         0    0     0     0  \n1       0       0     0         0             0         0    0     0     0  \n2       0       0     0         0             0         0    0     0     0  \n3       0       0     0         0             0         0    0     0     0  \n4       0       0     0         0             0         0    0     0     0  \n\n[5 rows x 15634 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Label</th>\n      <th>SMS</th>\n      <th>07808726822</th>\n      <th>087187262701</th>\n      <th>jokes</th>\n      <th>antelope</th>\n      <th>m6</th>\n      <th>doing</th>\n      <th>thurs</th>\n      <th>smells</th>\n      <th>...</th>\n      <th>bookedthe</th>\n      <th>tonite</th>\n      <th>hamper</th>\n      <th>sent</th>\n      <th>fishhead</th>\n      <th>disconnected</th>\n      <th>tomorrow</th>\n      <th>180</th>\n      <th>2wks</th>\n      <th>diff</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>[squeeeeeze, this, is, christmas, hug, if, u, ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>[and, also, i, ve, sorta, blown, him, off, a, ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ham</td>\n      <td>[mmm, thats, better, now, i, got, a, roast, do...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>[mm, have, some, kanji, dont, eat, anything, h...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>[so, there, s, a, ring, that, comes, with, the...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 15634 columns</p>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "On-SEF1fdd-Q"
   },
   "source": [
    "### Значения для формулы Байеса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем необходимые значения для формулы Байеса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "IwxBHjXYdd-Q",
    "ExecuteTime": {
     "end_time": "2023-07-26T06:06:37.536701700Z",
     "start_time": "2023-07-26T06:06:37.523700100Z"
    }
   },
   "outputs": [],
   "source": [
    "alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "gxsFfXOzdd-Q",
    "ExecuteTime": {
     "end_time": "2023-07-26T06:06:38.802733200Z",
     "start_time": "2023-07-26T06:06:38.759735600Z"
    }
   },
   "outputs": [],
   "source": [
    "Nvoc = len(vocabulary)\n",
    "Pspam = train_data['Label'].value_counts()['spam'] / train_data.shape[0]\n",
    "Pham = train_data['Label'].value_counts()['ham'] / train_data.shape[0]\n",
    "Nspam = train_data.loc[train_data['Label'] == 'spam', 'SMS'].apply(len).sum()\n",
    "Nham = train_data.loc[train_data['Label'] == 'ham', 'SMS'].apply(len).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "jde0BGdPdd-R",
    "ExecuteTime": {
     "end_time": "2023-07-26T06:06:41.901296300Z",
     "start_time": "2023-07-26T06:06:41.891293400Z"
    }
   },
   "outputs": [],
   "source": [
    "def p_w_spam(word):\n",
    "    if word in train_data.columns:\n",
    "        return (train_data.loc[train_data['Label'] == 'spam', word].sum() + alpha) / (Nspam + alpha*Nvoc)\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def p_w_ham(word):\n",
    "    if word in train_data.columns:\n",
    "        return (train_data.loc[train_data['Label'] == 'ham', word].sum() + alpha) / (Nham + alpha*Nvoc)\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-nPavNOdd-S"
   },
   "source": [
    "### Готовим алгоритм классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Ib39OrM8dd-S",
    "ExecuteTime": {
     "end_time": "2023-07-26T06:06:48.616992100Z",
     "start_time": "2023-07-26T06:06:48.609992Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify(message):\n",
    "    p_spam_given_message = Pspam\n",
    "    p_ham_given_message = Pham\n",
    "    for word in message:\n",
    "        p_spam_given_message *= p_w_spam(word)\n",
    "        p_ham_given_message *= p_w_ham(word)\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'классификация некорректна'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QECGrw8dd-S"
   },
   "source": [
    "### Используем тестовые данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "dNWXV-5odd-S",
    "ExecuteTime": {
     "end_time": "2023-07-26T06:06:56.534101500Z",
     "start_time": "2023-07-26T06:06:53.170757Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lagovskiy.sergey\\AppData\\Local\\Temp\\1\\ipykernel_18876\\1651366482.py:5: FutureWarning: reindexing with a non-unique Index is deprecated and will raise in a future version.\n",
      "  p_spam_given_message *= p_w_spam(word)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reindex on an axis with duplicate labels",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[27], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m test_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpredicted\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mtest_data\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mSMS\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclassify\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32md:\\Work\\DST\\__environment\\venv\\lib\\site-packages\\pandas\\core\\series.py:4539\u001B[0m, in \u001B[0;36mSeries.map\u001B[1;34m(self, arg, na_action)\u001B[0m\n\u001B[0;32m   4460\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmap\u001B[39m(\n\u001B[0;32m   4461\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   4462\u001B[0m     arg: Callable \u001B[38;5;241m|\u001B[39m Mapping \u001B[38;5;241m|\u001B[39m Series,\n\u001B[0;32m   4463\u001B[0m     na_action: Literal[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   4464\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Series:\n\u001B[0;32m   4465\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   4466\u001B[0m \u001B[38;5;124;03m    Map values of Series according to an input mapping or function.\u001B[39;00m\n\u001B[0;32m   4467\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4537\u001B[0m \u001B[38;5;124;03m    dtype: object\u001B[39;00m\n\u001B[0;32m   4538\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 4539\u001B[0m     new_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_values\u001B[49m\u001B[43m(\u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_action\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4540\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_constructor(new_values, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex)\u001B[38;5;241m.\u001B[39m__finalize__(\n\u001B[0;32m   4541\u001B[0m         \u001B[38;5;28mself\u001B[39m, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmap\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   4542\u001B[0m     )\n",
      "File \u001B[1;32md:\\Work\\DST\\__environment\\venv\\lib\\site-packages\\pandas\\core\\base.py:890\u001B[0m, in \u001B[0;36mIndexOpsMixin._map_values\u001B[1;34m(self, mapper, na_action)\u001B[0m\n\u001B[0;32m    887\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n\u001B[0;32m    889\u001B[0m \u001B[38;5;66;03m# mapper is a function\u001B[39;00m\n\u001B[1;32m--> 890\u001B[0m new_values \u001B[38;5;241m=\u001B[39m \u001B[43mmap_f\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    892\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m new_values\n",
      "File \u001B[1;32md:\\Work\\DST\\__environment\\venv\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[1;34m()\u001B[0m\n",
      "Cell \u001B[1;32mIn[26], line 5\u001B[0m, in \u001B[0;36mclassify\u001B[1;34m(message)\u001B[0m\n\u001B[0;32m      3\u001B[0m p_ham_given_message \u001B[38;5;241m=\u001B[39m Pham\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m message:\n\u001B[1;32m----> 5\u001B[0m     p_spam_given_message \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m p_w_spam(word)\n\u001B[0;32m      6\u001B[0m     p_ham_given_message \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m p_w_ham(word)\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m p_ham_given_message \u001B[38;5;241m>\u001B[39m p_spam_given_message:\n",
      "File \u001B[1;32md:\\Work\\DST\\__environment\\venv\\lib\\site-packages\\pandas\\core\\generic.py:12104\u001B[0m, in \u001B[0;36mNDFrame.__imul__\u001B[1;34m(self, other)\u001B[0m\n\u001B[0;32m  12102\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__imul__\u001B[39m(\u001B[38;5;28mself\u001B[39m: NDFrameT, other) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m NDFrameT:\n\u001B[0;32m  12103\u001B[0m     \u001B[38;5;66;03m# error: Unsupported left operand type for * (\"Type[NDFrame]\")\u001B[39;00m\n\u001B[1;32m> 12104\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inplace_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mtype\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__mul__\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32md:\\Work\\DST\\__environment\\venv\\lib\\site-packages\\pandas\\core\\generic.py:12090\u001B[0m, in \u001B[0;36mNDFrame._inplace_method\u001B[1;34m(self, other, op)\u001B[0m\n\u001B[0;32m  12085\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset_cacher()\n\u001B[0;32m  12087\u001B[0m \u001B[38;5;66;03m# this makes sure that we are aligned like the input\u001B[39;00m\n\u001B[0;32m  12088\u001B[0m \u001B[38;5;66;03m# we are updating inplace so we want to ignore is_copy\u001B[39;00m\n\u001B[0;32m  12089\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_inplace(\n\u001B[1;32m> 12090\u001B[0m     \u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreindex_like\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m, verify_is_copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m  12091\u001B[0m )\n\u001B[0;32m  12092\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32md:\\Work\\DST\\__environment\\venv\\lib\\site-packages\\pandas\\core\\generic.py:4431\u001B[0m, in \u001B[0;36mNDFrame.reindex_like\u001B[1;34m(self, other, method, copy, limit, tolerance)\u001B[0m\n\u001B[0;32m   4330\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   4331\u001B[0m \u001B[38;5;124;03mReturn an object with matching indices as other object.\u001B[39;00m\n\u001B[0;32m   4332\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4421\u001B[0m \u001B[38;5;124;03m2014-02-15          35.1              NaN    medium\u001B[39;00m\n\u001B[0;32m   4422\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   4423\u001B[0m d \u001B[38;5;241m=\u001B[39m other\u001B[38;5;241m.\u001B[39m_construct_axes_dict(\n\u001B[0;32m   4424\u001B[0m     axes\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_AXIS_ORDERS,\n\u001B[0;32m   4425\u001B[0m     method\u001B[38;5;241m=\u001B[39mmethod,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4428\u001B[0m     tolerance\u001B[38;5;241m=\u001B[39mtolerance,\n\u001B[0;32m   4429\u001B[0m )\n\u001B[1;32m-> 4431\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreindex(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39md)\n",
      "File \u001B[1;32md:\\Work\\DST\\__environment\\venv\\lib\\site-packages\\pandas\\core\\series.py:5094\u001B[0m, in \u001B[0;36mSeries.reindex\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   5090\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[0;32m   5091\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mindex\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m passed as both positional and keyword argument\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   5092\u001B[0m         )\n\u001B[0;32m   5093\u001B[0m     kwargs\u001B[38;5;241m.\u001B[39mupdate({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindex\u001B[39m\u001B[38;5;124m\"\u001B[39m: index})\n\u001B[1;32m-> 5094\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mreindex(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32md:\\Work\\DST\\__environment\\venv\\lib\\site-packages\\pandas\\core\\generic.py:5289\u001B[0m, in \u001B[0;36mNDFrame.reindex\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   5286\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reindex_multi(axes, copy, fill_value)\n\u001B[0;32m   5288\u001B[0m \u001B[38;5;66;03m# perform the reindex on the axes\u001B[39;00m\n\u001B[1;32m-> 5289\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reindex_axes\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   5290\u001B[0m \u001B[43m    \u001B[49m\u001B[43maxes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlimit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtolerance\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfill_value\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\n\u001B[0;32m   5291\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39m__finalize__(\u001B[38;5;28mself\u001B[39m, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreindex\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32md:\\Work\\DST\\__environment\\venv\\lib\\site-packages\\pandas\\core\\generic.py:5309\u001B[0m, in \u001B[0;36mNDFrame._reindex_axes\u001B[1;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001B[0m\n\u001B[0;32m   5304\u001B[0m new_index, indexer \u001B[38;5;241m=\u001B[39m ax\u001B[38;5;241m.\u001B[39mreindex(\n\u001B[0;32m   5305\u001B[0m     labels, level\u001B[38;5;241m=\u001B[39mlevel, limit\u001B[38;5;241m=\u001B[39mlimit, tolerance\u001B[38;5;241m=\u001B[39mtolerance, method\u001B[38;5;241m=\u001B[39mmethod\n\u001B[0;32m   5306\u001B[0m )\n\u001B[0;32m   5308\u001B[0m axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_axis_number(a)\n\u001B[1;32m-> 5309\u001B[0m obj \u001B[38;5;241m=\u001B[39m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reindex_with_indexers\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   5310\u001B[0m \u001B[43m    \u001B[49m\u001B[43m{\u001B[49m\u001B[43maxis\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mnew_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m]\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   5311\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfill_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfill_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   5312\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   5313\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_dups\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   5314\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   5315\u001B[0m \u001B[38;5;66;03m# If we've made a copy once, no need to make another one\u001B[39;00m\n\u001B[0;32m   5316\u001B[0m copy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32md:\\Work\\DST\\__environment\\venv\\lib\\site-packages\\pandas\\core\\generic.py:5355\u001B[0m, in \u001B[0;36mNDFrame._reindex_with_indexers\u001B[1;34m(self, reindexers, fill_value, copy, allow_dups)\u001B[0m\n\u001B[0;32m   5352\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m ensure_platform_int(indexer)\n\u001B[0;32m   5354\u001B[0m \u001B[38;5;66;03m# TODO: speed up on homogeneous DataFrame objects (see _reindex_multi)\u001B[39;00m\n\u001B[1;32m-> 5355\u001B[0m new_data \u001B[38;5;241m=\u001B[39m \u001B[43mnew_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreindex_indexer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   5356\u001B[0m \u001B[43m    \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   5357\u001B[0m \u001B[43m    \u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   5358\u001B[0m \u001B[43m    \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbaxis\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   5359\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfill_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfill_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   5360\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_dups\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mallow_dups\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   5361\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   5362\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   5363\u001B[0m \u001B[38;5;66;03m# If we've made a copy once, no need to make another one\u001B[39;00m\n\u001B[0;32m   5364\u001B[0m copy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32md:\\Work\\DST\\__environment\\venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py:737\u001B[0m, in \u001B[0;36mBaseBlockManager.reindex_indexer\u001B[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001B[0m\n\u001B[0;32m    735\u001B[0m \u001B[38;5;66;03m# some axes don't allow reindexing with dups\u001B[39;00m\n\u001B[0;32m    736\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m allow_dups:\n\u001B[1;32m--> 737\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maxes\u001B[49m\u001B[43m[\u001B[49m\u001B[43maxis\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_can_reindex\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    739\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m axis \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mndim:\n\u001B[0;32m    740\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRequested axis not found in manager\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32md:\\Work\\DST\\__environment\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:4316\u001B[0m, in \u001B[0;36mIndex._validate_can_reindex\u001B[1;34m(self, indexer)\u001B[0m\n\u001B[0;32m   4314\u001B[0m \u001B[38;5;66;03m# trying to reindex on an axis with duplicates\u001B[39;00m\n\u001B[0;32m   4315\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_index_as_unique \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(indexer):\n\u001B[1;32m-> 4316\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcannot reindex on an axis with duplicate labels\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: cannot reindex on an axis with duplicate labels"
     ]
    }
   ],
   "source": [
    "test_data['predicted'] = test_data['SMS'].map(classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "AejY85XOdd-S",
    "outputId": "112fe436-4d36-42a6-8d3d-8952e282939c",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-07-26T06:07:12.600158900Z",
     "start_time": "2023-07-26T06:07:12.565129900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  Label                                                SMS\n0   ham  [u, dun, say, so, early, hor, u, c, already, t...\n1   ham  [nah, i, don, t, think, he, goes, to, usf, he,...\n2  spam  [freemsg, hey, there, darling, it, s, been, 3,...\n3  spam  [had, your, mobile, 11, months, or, more, u, r...\n4   ham                      [oh, k, i, m, watching, here]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Label</th>\n      <th>SMS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>[u, dun, say, so, early, hor, u, c, already, t...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>[nah, i, don, t, think, he, goes, to, usf, he,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>[freemsg, hey, there, darling, it, s, been, 3,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>spam</td>\n      <td>[had, your, mobile, 11, months, or, more, u, r...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>[oh, k, i, m, watching, here]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "lNHidizgdd-T",
    "ExecuteTime": {
     "end_time": "2023-07-26T06:07:24.511253600Z",
     "start_time": "2023-07-26T06:07:24.009980300Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'predicted'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32md:\\Work\\DST\\__environment\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3801\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3802\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3803\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32md:\\Work\\DST\\__environment\\venv\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32md:\\Work\\DST\\__environment\\venv\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'predicted'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[29], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m correct \u001B[38;5;241m=\u001B[39m (\u001B[43mtest_data\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpredicted\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m \u001B[38;5;241m==\u001B[39m test_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLabel\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39msum() \u001B[38;5;241m/\u001B[39m test_data\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mПравильных предсказаний \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcorrect\u001B[38;5;250m \u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m100\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m %\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32md:\\Work\\DST\\__environment\\venv\\lib\\site-packages\\pandas\\core\\frame.py:3807\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3805\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   3806\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[1;32m-> 3807\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3808\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[0;32m   3809\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[1;32md:\\Work\\DST\\__environment\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3802\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[0;32m   3803\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m-> 3804\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m   3805\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m   3806\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[0;32m   3807\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[0;32m   3808\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[0;32m   3809\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'predicted'"
     ]
    }
   ],
   "source": [
    "correct = (test_data['predicted'] == test_data['Label']).sum() / test_data.shape[0]\n",
    "print(f\"Правильных предсказаний {correct * 100:3f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "-s5NRlJGdd-T",
    "outputId": "de7985cf-9878-486c-ff7a-54295961199e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>[freemsg, hey, there, darling, it, s, been, 3,...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>ham</td>\n",
       "      <td>[waiting, for, your, call]</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>ham</td>\n",
       "      <td>[26th, of, july]</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>spam</td>\n",
       "      <td>[sms, ac, jsco, energy, is, high, but, u, may,...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>ham</td>\n",
       "      <td>[the, last, thing, i, ever, wanted, to, do, wa...</td>\n",
       "      <td>классификация некорректна</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Label                                                SMS  \\\n",
       "2    spam  [freemsg, hey, there, darling, it, s, been, 3,...   \n",
       "96    ham                         [waiting, for, your, call]   \n",
       "182   ham                                   [26th, of, july]   \n",
       "269  spam  [sms, ac, jsco, energy, is, high, but, u, may,...   \n",
       "344   ham  [the, last, thing, i, ever, wanted, to, do, wa...   \n",
       "\n",
       "                     predicted  \n",
       "2                          ham  \n",
       "96                        spam  \n",
       "182                       spam  \n",
       "269                        ham  \n",
       "344  классификация некорректна  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.loc[test_data['predicted'] != test_data['Label']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Наивный байесовский классификатор в sklearn\n",
    "Ура, мы реализовали наивный байесовский классификатор с нуля!\n",
    "А теперь посмотрим, как то же самое можно сделать с помощью библиотеки scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "V3xAvBRndd-T",
    "ExecuteTime": {
     "end_time": "2023-07-26T06:07:52.150759700Z",
     "start_time": "2023-07-26T06:07:44.107456200Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитаем заново csv-файл и предобработаем данные. Разбивать сообщения на слова в этот раз не нужно, мы сделаем это далее с помощью встроенных инструментов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-26T06:07:54.233491600Z",
     "start_time": "2023-07-26T06:07:54.155462500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  Label                                                SMS\n0   ham  go until jurong point crazy available only in ...\n1   ham                            ok lar joking wif u oni\n2  spam  free entry in 2 a wkly comp to win fa cup fina...\n3   ham        u dun say so early hor u c already then say\n4   ham  nah i don t think he goes to usf he lives arou...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Label</th>\n      <th>SMS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>go until jurong point crazy available only in ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>ok lar joking wif u oni</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>u dun say so early hor u c already then say</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>nah i don t think he goes to usf he lives arou...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    \"data/SMSSpamCollection.zip\", header=None, sep=\"\\t\", names=[\"Label\", \"SMS\"]\n",
    ")\n",
    "\n",
    "df[\"SMS\"] = df[\"SMS\"].str.replace(r\"\\W+\", \" \", regex=True).str.lower()\n",
    "\n",
    "df['SMS'] = df['SMS'].str.replace('\\s+', ' ', regex=True).str.strip()\n",
    "df['SMS'] = df['SMS'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем строки в векторный вид – то есть, снова создадим таблицу с частотами слов. Но в этот раз воспользуемся встроенным в sklearn классов CountVectorizer()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-26T06:07:57.278820400Z",
     "start_time": "2023-07-26T06:07:56.889101100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 8713) (5572,)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"SMS\"])\n",
    "y = df[\"Label\"]\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью функции `train_test_split` из scikit-learn разобьём выборку на обучающую и тестовую в пропорции 80/20. Не забудем сделать стратификацию!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-26T06:08:01.822372200Z",
     "start_time": "2023-07-26T06:08:01.778343200Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-26T06:08:07.680179300Z",
     "start_time": "2023-07-26T06:08:07.643180600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.979372197309417\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred)}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "НаивныйБайес.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "86c56a74836ad344b00594bf6f38fa6a676a207ceefe20d101fbc465800ccb8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
